{"cells":[{"cell_type":"markdown","id":"3bfd5247","metadata":{"id":"3bfd5247"},"source":["# Deep Deterministic Policy Gradient"]},{"cell_type":"code","source":["!pip install gymnasium\n","!pip install gymnasium[mujoco]\n","!pip install omegaconf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l_JAsBVqeG2l","executionInfo":{"status":"ok","timestamp":1685500224963,"user_tz":-540,"elapsed":9915,"user":{"displayName":"윤민서","userId":"16983864168157635834"}},"outputId":"334e0626-c7af-4600-c18d-e20035c1b665"},"id":"l_JAsBVqeG2l","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.28.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.22.4)\n","Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.0.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.10/dist-packages (0.28.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (1.22.4)\n","Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (1.0.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (0.0.4)\n","Requirement already satisfied: mujoco>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.3.5)\n","Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.25.1)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (8.4.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.2->gymnasium[mujoco]) (1.4.0)\n","Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.2->gymnasium[mujoco]) (2.5.9)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.2->gymnasium[mujoco]) (3.1.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0)\n"]}]},{"cell_type":"code","execution_count":2,"id":"fc525f8c","metadata":{"id":"fc525f8c","executionInfo":{"status":"ok","timestamp":1685500227392,"user_tz":-540,"elapsed":2434,"user":{"displayName":"윤민서","userId":"16983864168157635834"}}},"outputs":[],"source":["import copy\n","import random\n","import collections\n","\n","import numpy as np\n","import gymnasium as gym\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from omegaconf import OmegaConf\n","from gymnasium.experimental.wrappers import RecordVideoV0 as RecordVideo\n"]},{"cell_type":"markdown","id":"3ed8b3a2","metadata":{"id":"3ed8b3a2"},"source":["# Environment"]},{"cell_type":"code","execution_count":19,"id":"05f0ab82","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"05f0ab82","executionInfo":{"status":"ok","timestamp":1685500644972,"user_tz":-540,"elapsed":752,"user":{"displayName":"윤민서","userId":"16983864168157635834"}},"outputId":"f1ada5e7-73b9-42f8-961a-cb16dcda46c2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<TimeLimit<OrderEnforcing<PassiveEnvChecker<HalfCheetahEnv<HalfCheetah-v4>>>>>"]},"metadata":{},"execution_count":19}],"source":["# continuous environment\n","env = gym.make('HalfCheetah-v4', render_mode=\"rgb_array\")\n","# env = RecordVideo(env, \"./videos\", disable_logger=False)\n","env"]},{"cell_type":"markdown","id":"f48fe096","metadata":{"id":"f48fe096"},"source":["# Hyperparameters"]},{"cell_type":"code","execution_count":20,"id":"c1831b41","metadata":{"id":"c1831b41","executionInfo":{"status":"ok","timestamp":1685500645802,"user_tz":-540,"elapsed":2,"user":{"displayName":"윤민서","userId":"16983864168157635834"}}},"outputs":[],"source":["AC_config = OmegaConf.create({\n","    # RL parameter\n","    'gamma': 0.99,\n","    \n","    # replay memory\n","    'buffer_limit': int(1e5),\n","    'batch_size': 32,\n","    \n","    # neural network parameters\n","    'device': 'cuda:0',\n","    'hidden_dim': 64,\n","    'state_dim': env.observation_space.shape[0],\n","    'action_dim': int(env.action_space.shape[0]), # cannot use .n because not actions are continuous!\n","    \n","    # learning parameters\n","    'lr_actor': 0.0005,\n","    'lr_critic': 0.001,\n","    'tau': 0.005,\n","})"]},{"cell_type":"markdown","id":"a7e486d4","metadata":{"id":"a7e486d4"},"source":["# Special functions\n","- Replay Buffer\n","- Ornstein_Uhlenbeck_Noise: add noise to the action (=output of Actor network) $\\rightarrow$ exploration $\\uparrow$\n","- soft_update: prevent the drastic change of neural network"]},{"cell_type":"code","execution_count":21,"id":"9dfa5279","metadata":{"id":"9dfa5279","executionInfo":{"status":"ok","timestamp":1685500645802,"user_tz":-540,"elapsed":2,"user":{"displayName":"윤민서","userId":"16983864168157635834"}}},"outputs":[],"source":["# replay buffer\n","class ReplayBuffer():\n","    def __init__(self, config):\n","        self.config = config\n","        self.buffer = collections.deque(maxlen=self.config.buffer_limit)\n","\n","    def put(self, transition):\n","        self.buffer.append(transition)\n","    \n","    def sample(self, n):\n","        mini_batch = random.sample(self.buffer, n)\n","        s_lst, a_lst, r_lst, next_s_lst, done_mask_lst = [], [], [], [], []\n","\n","        for transition in mini_batch:\n","            s, a, r, next_s, done = transition\n","            s_lst.append(s.tolist())\n","            a_lst.append(a.tolist())\n","            r_lst.append([r])\n","            next_s_lst.append(next_s.tolist())\n","            done_mask = 0.0 if done else 1.0 \n","            done_mask_lst.append([done_mask])\n","        \n","        return torch.Tensor(s_lst), torch.Tensor(a_lst), torch.Tensor(r_lst), torch.Tensor(next_s_lst), torch.Tensor(done_mask_lst)\n","    \n","    def size(self):\n","        return len(self.buffer)\n","    \n","    \n","class OrnsteinUhlenbeckNoise:\n","    def __init__(self, mu):\n","        self.theta, self.dt, self.sigma = 0.1, 0.01, 0.1\n","        self.mu = mu\n","        self.x_prev = np.zeros_like(self.mu)\n","\n","    def __call__(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n","                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n","        self.x_prev = x\n","        return x\n","    \n","# moving average over the neural network parameters\n","def soft_update(net, net_target, tau):\n","    # for each parameters,\n","    for param_target, param in zip(net_target.parameters(), net.parameters()):\n","        # mix the target and current parameters with the ratio of (1 - tau) : (tau)\n","        param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)\n","    "]},{"cell_type":"markdown","id":"37a272ff","metadata":{"id":"37a272ff"},"source":["# Main Structure: Actor-Critic\n","- with target networks (recall DQN)\n","- update method has changed:\n"]},{"cell_type":"code","execution_count":22,"id":"e50bb794","metadata":{"id":"e50bb794","executionInfo":{"status":"ok","timestamp":1685500646470,"user_tz":-540,"elapsed":7,"user":{"displayName":"윤민서","userId":"16983864168157635834"}}},"outputs":[],"source":["class ActorCritic(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.data = []\n","        self.config = config\n","        \n","        # create replay buffer\n","        self.memory = ReplayBuffer(self.config)\n","        # set exploration noise\n","        self.action_noise = OrnsteinUhlenbeckNoise(mu=np.zeros(self.config.action_dim))\n","\n","        # actor: policy network\n","        self.actor = nn.Sequential(\n","            nn.Linear(self.config.state_dim, self.config.hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(self.config.hidden_dim, self.config.hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(self.config.hidden_dim, self.config.action_dim),\n","            nn.Tanh(), # continuous action, bound output to [-1, 1]\n","        )\n","        # critic: Q(s, a) network\n","        self.critic = nn.Sequential(\n","            nn.Linear(self.config.state_dim + self.config.action_dim, self.config.hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(self.config.hidden_dim, self.config.hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(self.config.hidden_dim, 1),\n","        )\n","        # we need target networks:\n","        self.actor_target, self.critic_target = copy.deepcopy(self.actor), copy.deepcopy(self.critic)\n","        \n","        # load them to gpu (if available)\n","        self.to(self.config.device)\n","        \n","        # we use different learning rates for actor and critic networks\n","        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=self.config.lr_actor)\n","        self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr=self.config.lr_critic)\n","        \n","        # parameter for soft update\n","        self.tau = self.config.tau\n","        \n","    # training function\n","    def update(self):\n","        # randomly sample from replay buffer\n","        states, actions, rewards, next_states, dones = self.memory.sample(self.config.batch_size)\n","        states = torch.Tensor(states).to(AC_config['device'])\n","        actions = torch.Tensor(actions).to(AC_config['device'])\n","        rewards = torch.Tensor(rewards).to(AC_config['device'])\n","        next_states = torch.Tensor(next_states).to(AC_config['device'])\n","        dones = torch.Tensor(dones).to(AC_config['device'])\n","        \n","        # compute target q values -- we concatenate state & action to make (s, a) \n","        # they have shape of (Batch size x state_dim) & (Batch size x action_dim).\n","        # We need to make it (Batch size x state_dim + action_dim), meaning that the concatenation must happen in the last dimension,\n","        # i.e. dim=-1\n","        target_q_values = rewards + self.config.gamma * self.critic_target(\n","            torch.cat([next_states, self.actor_target(next_states)], dim=-1)\n","        ) * dones\n","        \n","        # compute q loss\n","        critic_loss = F.smooth_l1_loss(self.critic(torch.cat([states, actions], dim=-1)), target_q_values.detach())\n","        # compute gradient & update\n","        self.critic_opt.zero_grad()\n","        critic_loss.backward()\n","        self.critic_opt.step()\n","\n","        actor_loss = -self.critic(torch.cat([states, self.actor(states)], dim=-1)).mean() # That's all for the policy loss.\n","        self.actor_opt.zero_grad()\n","        actor_loss.backward()\n","        self.actor_opt.step()\n","        \n","        # soft update\n","        soft_update(self.actor, self.actor_target, self.tau)\n","        soft_update(self.critic, self.critic_target, self.tau)\n","        "]},{"cell_type":"markdown","id":"870aad66","metadata":{"id":"870aad66"},"source":["# Learn"]},{"cell_type":"code","execution_count":null,"id":"08ca5394","metadata":{"id":"08ca5394","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a33581d4-f2a5-449c-d4e4-403adf291aaa"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 10%|▉         | 489/5000 [06:43<1:01:15,  1.23it/s]"]}],"source":["num_epis, epi_rews = 5000, []\n","agent = ActorCritic(AC_config)\n","\n","for n_epi in tqdm(range(num_epis)):\n","    state, _ = env.reset()\n","    terminated, truncated = False, False\n","    epi_rew = 0\n","    \n","    while not (terminated or truncated):\n","        # get action from actor network\n","        action = agent.actor(torch.Tensor(list(state)).to(AC_config.device))\n","        # add noise for better exploration\n","        action = action + torch.Tensor(agent.action_noise()).to(AC_config.device)\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action.detach().cpu().numpy())\n","\n","        # save transition to replay buffer\n","        agent.memory.put((state, action, reward, next_state, terminated or truncated))\n","\n","        # state transition\n","        state = next_state\n","\n","        # record reward\n","        epi_rew += reward\n","        \n","    # enough memory\n","    if agent.memory.size() > 5000:\n","        # off-line training\n","        for i in range(10):\n","            agent.update()\n","            \n","    epi_rews += [epi_rew]\n","env.close()"]},{"cell_type":"code","execution_count":null,"id":"84912e1c","metadata":{"id":"84912e1c"},"outputs":[],"source":["plt.figure(figsize=(20, 10), dpi=300)\n","plt.plot(epi_rews, label='episode returns')\n","plt.legend(fontsize=20)\n","plt.show()\n","plt.close()"]},{"cell_type":"markdown","id":"b251cfc7","metadata":{"id":"b251cfc7"},"source":["# Check the video!"]},{"cell_type":"code","execution_count":null,"id":"ad188169","metadata":{"id":"ad188169","executionInfo":{"status":"aborted","timestamp":1685500227395,"user_tz":-540,"elapsed":8,"user":{"displayName":"윤민서","userId":"16983864168157635834"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"vscode":{"interpreter":{"hash":"5c6276770e9c5a685155d297a75ee9fe2216c271da1ff18b89edbaa80fe1643b"}},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}