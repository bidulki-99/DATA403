{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Basic  \n",
    "![](_2023-04-17-13-59-20.png)  \n",
    "\n",
    "작성자: 김국진 kukjinkim@korea.ac.kr  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](_2023-04-17-13-37-23.png)  \n",
    "\n",
    "과연 ChatGPT는 Pytorch로 구현되었을까요?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- Class Review \n",
    "- Data representation with tensor <-\n",
    "- Tensor Manipulation <-\n",
    "- Dataset, DataLoader\n",
    "- Regresseion \n",
    "- Clasification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2016년 이후로 딥러닝과 강화학습은 정말 빠른 속도로 발전하여 현재의 ChatGPT와 같은 혁신적인 결과물을 탄생시켰습니다.  \n",
    "본 실습에서는 아주 널리 쓰이고 있는 Pytorch 딥러닝 프레임워크에 대해 본격적으로 실습해보겠습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습을 위해 GPU가 필요합니다. 노트북에 GPU가 없다면, 모든 device 코드를 cpu로 바꿔서 실행하면 해결할 수 있습니다. 또는 Colab 상에서 실습을 진행해주세요. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](2023-04-25-21-42-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-04-25-21-43-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data representation with tensor  \n",
    "선형대수에서는 스칼라, 벡터, 행렬, 텐서의 개념에 대해 배웁니다.  \n",
    "선형대수 책과 많은 머신러닝 도서들에서는 아래와 같은 기호로 이들을 표기합니다.  \n",
    "여러 모듈과 메소드를 통해 파이토치에서 어떻게 텐서를 조작하는지 알아보겠습니다  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Scalar} : x\\quad 0 \\  \\text{(0-dimension) }$$\n",
    "$$\\text{Vector} : \\mathbf x \\quad n \\ \\text{ (1-dimension)}$$\n",
    "$$\\text{Matrix} : \\mathbf X \\quad m \\times n \\  \\text {(2-dimension)}$$\n",
    "$$\\text{Tensor} : \\mathcal {X} \\quad l \\times m \\times n \\times \\dots \\text {(more 3-dimension)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-04-25-21-44-38.png)\n",
    "\n",
    "(https://furkangulsen.medium.com/what-is-a-tensor-ce8e78835d08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프로그래밍 과목의 초입에서 배우는 데이터의 타입을 떠올려보겠습니다.  \n",
    "대표적으로 다음과 같습니다. \n",
    "1. Int  \n",
    "2. Double  \n",
    "3. Float  \n",
    "\n",
    "마찬가지로 torch의 여러가지 텐서모듈을 통해 위의 데이터타입을 가지는 텐서를 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "from torch import FloatTensor as ftensor\n",
    "from torch import DoubleTensor as dtensor\n",
    "from torch import IntTensor as itensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Scalar\n",
    "`torch.tensor(data)`는 data array을 입력으로 받아서 pytorch의 tensor object를 리턴합니다.  \n",
    "  \n",
    "  \n",
    "텐서 객체는 다음과 같은 형태로 할당하게 됩니다.    \n",
    "`tensor = torch.tensor(data, dtype=float32, device='cpu')`   \n",
    "  \n",
    "\n",
    "텐서 객체를 생성하기 위한 파라미터와 인자는 data, dtype, device가 있습니다.   \n",
    "data에는 리스트나 numpy 배열이 들어갈 수 있으며,  \n",
    "device에는 cpu나 cuda:0와 같이 gpu 인덱스가 들어갈 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로는 argument를 주지 않으면 tensor는 cpu로 계산되고, data type은 float32나 int64로 설정됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "cpu\n",
      "torch.float32\n",
      "tensor(2, device='cuda:0')\n",
      "cuda:0\n",
      "torch.int64\n",
      "1.0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor(1.0)\n",
    "print(tensor1)\n",
    "print(tensor1.device)\n",
    "print(tensor1.dtype)\n",
    "\n",
    "tensor2 = torch.tensor(2, device='cuda')\n",
    "print(tensor2)\n",
    "print(tensor2.device)\n",
    "print(tensor2.dtype)\n",
    "\n",
    "# For plotting value\n",
    "print(tensor1.item())\n",
    "print(tensor2.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 기억해야할 것들은 다음과 같습니다. \n",
    "- 텐서 오브젝트의 dtype과 device는 모두 통일시켜야 한다\n",
    "- 즉, 입력 텐서의 데이터타입과 모델 가중치 텐서의 데이터 타입이 같아야 한다. \n",
    "- 또한, 입력 텐서와 모델 가중치 텐서의 계산장치가 같아야 한다.\n",
    "\n",
    "위 사항들을 지켜지지 않았을 때는 pytorch가 내부적으로 device와 dtype을 변환해서 연산을 수행해줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., device='cuda:0')\n",
      "cuda:0\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "tensor3 = tensor1 + tensor2\n",
    "print(tensor3)\n",
    "print(tensor3.device)\n",
    "print(tensor3.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vec = np.array([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy에서는 시퀀스 데이터(리스트, 튜플)을 넣어서 벡터나 텐서를 표현합니다.  \n",
    "`torch.tensor(data)`에서 data에 들어갈 수 있는 컨테이너는 List, Tuple, Numpy array입니다.  \n",
    "파이토치에서 텐서 객체를 만드는 방법은 기존의 numpy와 매우 유사합니다.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0000, -1.1000,  3.9000])\n",
      "torch.float32\n",
      "tensor([ 1.0000, -1.1000,  3.9000], dtype=torch.float64)\n",
      "tensor([1, 2, 3, 4], dtype=torch.int32)\n",
      "tensor([ 1, -1,  3])\n"
     ]
    }
   ],
   "source": [
    "vec1 = ftensor([1.0, -1.1, 3.9]) # list input\n",
    "vec2 = dtensor((1.0, -1.1, 3.9)) # Tuple input\n",
    "vec3 = tensor(np.array([1, 2, 3, 4])) # numpy ary input\n",
    "vec4 = tensor([1.0, -1.1, 3.9], dtype=int)\n",
    "\n",
    "print(vec1)\n",
    "print(vec1.dtype)\n",
    "print(vec2)\n",
    "print(vec3)\n",
    "print(vec4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Matrix  \n",
    "이제 행렬 텐서를 만들어보겠습니다.  \n",
    "데이터타입을 행렬로 다루기 시작한다면 병렬처리를 위해 device를 gpu로 지정하는게 좋습니다.  \n",
    "이제 데이터를 일일히 생성하기 힘드므로 rand를 사용하겠습니다.  \n",
    "rand의 결과가 같도록 만들기 위해 시드를 고정하겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b646b82610>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1) # torch.manual_seed(seed) fixing random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8903, 0.0275],\n",
      "        [0.9031, 0.5386]], device='cuda:0')\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "mat = torch.rand([2, 2], device='cuda')\n",
    "print(mat)\n",
    "print(mat.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Tensor\n",
    "이제 3차원 이상의 텐서를 만들어보겠습니다.  \n",
    "`to(dtype=dtype, device=device)` : 텐서 내의 메소드로 원하는 데이터 타입, 원하는 디바이스의 텐서를 반환해줍니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[157.3619,  97.5393, 163.1013],\n",
      "         [121.4620, 182.6800, 158.4733],\n",
      "         [113.2884,  24.5168, 157.2242],\n",
      "         [ 14.6759, 144.8228, 136.5066],\n",
      "         [ 99.8530, 232.6652, 136.5423],\n",
      "         [181.0788, 182.1689,  52.4888],\n",
      "         [ 78.7924, 251.0999,   2.6269],\n",
      "         [119.3065, 117.8560, 218.7916],\n",
      "         [115.8311, 161.7047, 121.8555],\n",
      "         [ 56.3268,  55.4509,  65.8133],\n",
      "         [ 11.7242,  44.9303, 158.1247],\n",
      "         [212.2427, 134.3056,  69.3261],\n",
      "         [184.2539,  78.8643,  99.6456],\n",
      "         [ 57.8295,  87.8077,   9.3952],\n",
      "         [182.6163, 177.7710, 153.4251],\n",
      "         [190.8456, 182.2505, 133.6631],\n",
      "         [141.5559, 137.7742, 196.3005],\n",
      "         [213.9836, 219.9226, 202.1895],\n",
      "         [ 96.7924, 122.3017, 101.9875],\n",
      "         [202.4617, 142.2154, 246.4687],\n",
      "         [192.9206,  18.6036, 165.4441],\n",
      "         [250.9919, 241.6952, 125.9885],\n",
      "         [170.4645,   7.9294,  87.1935],\n",
      "         [190.4178,  11.3967, 239.5140],\n",
      "         [ 43.8241, 168.4765, 123.1541],\n",
      "         [150.5558, 140.4013,   8.3414],\n",
      "         [100.5056,  47.0898, 236.8197],\n",
      "         [112.2881,   0.5481, 159.0015],\n",
      "         [183.5866,  70.6997, 115.9926],\n",
      "         [183.3357,  48.3575,  60.3371],\n",
      "         [115.6523,  38.1115, 206.6686],\n",
      "         [138.4693, 204.5829, 196.5255],\n",
      "         [ 29.3553,  48.2183,  40.4503],\n",
      "         [ 86.8607,  81.2257, 107.3753],\n",
      "         [  4.1727, 182.0540, 200.6152],\n",
      "         [168.5674, 209.3385, 224.1575],\n",
      "         [  1.6416, 147.3351, 246.7343],\n",
      "         [ 35.2363,  71.0074, 121.2593],\n",
      "         [ 48.3733, 103.8743, 109.0854],\n",
      "         [242.0443, 119.9515, 197.4108],\n",
      "         [221.7313, 185.0269,  16.6867],\n",
      "         [223.9366, 212.4096,  36.2417],\n",
      "         [ 82.3588, 215.1206,   3.5706],\n",
      "         [ 15.8123,  41.2469, 167.8847],\n",
      "         [ 75.7288,  13.8444, 177.6167],\n",
      "         [192.7454, 175.9440,  18.3331],\n",
      "         [252.6359, 118.3424,   6.1728],\n",
      "         [108.7307, 211.6037, 186.9477],\n",
      "         [126.6320, 218.2348,  11.2154],\n",
      "         [216.8122, 255.0596,  50.1677],\n",
      "         [155.4554, 107.3592,  19.9437],\n",
      "         [126.8779,  85.1043,  18.6695],\n",
      "         [ 34.7304, 130.7930, 246.6645],\n",
      "         [173.8281,  42.8369, 216.2877],\n",
      "         [138.5067,   2.9256, 134.0717],\n",
      "         [210.1685,  52.7353, 122.1105],\n",
      "         [ 64.2282,  27.0711,  55.2687],\n",
      "         [140.8536, 210.7410, 104.2147],\n",
      "         [ 12.8880, 126.9043,  16.6750],\n",
      "         [135.5190, 222.9021, 182.6284],\n",
      "         [ 49.7106,  99.7507, 179.2545],\n",
      "         [162.7243,   7.7663,  53.3739],\n",
      "         [ 82.7316, 175.0387, 242.3684],\n",
      "         [185.0385, 176.3532,  19.3843]]])\n",
      "torch.float32\n",
      "tensor([[[157,  97, 163],\n",
      "         [121, 182, 158],\n",
      "         [113,  24, 157],\n",
      "         [ 14, 144, 136],\n",
      "         [ 99, 232, 136],\n",
      "         [181, 182,  52],\n",
      "         [ 78, 251,   2],\n",
      "         [119, 117, 218],\n",
      "         [115, 161, 121],\n",
      "         [ 56,  55,  65],\n",
      "         [ 11,  44, 158],\n",
      "         [212, 134,  69],\n",
      "         [184,  78,  99],\n",
      "         [ 57,  87,   9],\n",
      "         [182, 177, 153],\n",
      "         [190, 182, 133],\n",
      "         [141, 137, 196],\n",
      "         [213, 219, 202],\n",
      "         [ 96, 122, 101],\n",
      "         [202, 142, 246],\n",
      "         [192,  18, 165],\n",
      "         [250, 241, 125],\n",
      "         [170,   7,  87],\n",
      "         [190,  11, 239],\n",
      "         [ 43, 168, 123],\n",
      "         [150, 140,   8],\n",
      "         [100,  47, 236],\n",
      "         [112,   0, 159],\n",
      "         [183,  70, 115],\n",
      "         [183,  48,  60],\n",
      "         [115,  38, 206],\n",
      "         [138, 204, 196],\n",
      "         [ 29,  48,  40],\n",
      "         [ 86,  81, 107],\n",
      "         [  4, 182, 200],\n",
      "         [168, 209, 224],\n",
      "         [  1, 147, 246],\n",
      "         [ 35,  71, 121],\n",
      "         [ 48, 103, 109],\n",
      "         [242, 119, 197],\n",
      "         [221, 185,  16],\n",
      "         [223, 212,  36],\n",
      "         [ 82, 215,   3],\n",
      "         [ 15,  41, 167],\n",
      "         [ 75,  13, 177],\n",
      "         [192, 175,  18],\n",
      "         [252, 118,   6],\n",
      "         [108, 211, 186],\n",
      "         [126, 218,  11],\n",
      "         [216, 255,  50],\n",
      "         [155, 107,  19],\n",
      "         [126,  85,  18],\n",
      "         [ 34, 130, 246],\n",
      "         [173,  42, 216],\n",
      "         [138,   2, 134],\n",
      "         [210,  52, 122],\n",
      "         [ 64,  27,  55],\n",
      "         [140, 210, 104],\n",
      "         [ 12, 126,  16],\n",
      "         [135, 222, 182],\n",
      "         [ 49,  99, 179],\n",
      "         [162,   7,  53],\n",
      "         [ 82, 175, 242],\n",
      "         [185, 176,  19]]], device='cuda:0')\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "# T = torch.rand([64, 64, 3], device='cuda:0')\n",
    "T = torch.rand([64, 64, 3], device='cpu')\n",
    "T = T * 256\n",
    "print(T[0:1])\n",
    "print(T.dtype)\n",
    "\n",
    "T = T.to(int) \n",
    "T = T.to(device='cuda:0')\n",
    "print(T[0:1])\n",
    "print(T.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pop Quiz 1 (Easy)  \n",
    "1) !nvidia-smi 를 통해 현재 gpu의 메모리 용량과 현재 사용량을 확인하세요\n",
    "2) 크기가 (1000, 64, 64, 3)인 4차원 랜덤 텐서를 생성해서 gpu 메모리에 할당하세요\n",
    "3) 다시 한 번 더 `!nvidia-smi` 를 통해 gpu 메모리 사용량이 얼마나 늘었는지 계산해보세요\n",
    "\n",
    "#### 만약 노트북에 GPU가 없다면 CPU를 device로 설정하고 작업관리자를 통해 메모리 사용량을 확인해보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4d_tensor = pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tensor manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형대수에서는 두 벡터의 덧셈, 뺄셈, 내적, 외적 등의 연산 방법을 배우고더 행렬의 덧셈, 뺄셈, 곱셈, 역행렬의 계산 등의 행렬 연산을 배웁니다. 이러한 모든 벡터, 행렬, 텐서 연산방법을 마찬가지로 pytorch에서도 수행할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Vector, Matrix, Tensor Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32)\n"
     ]
    }
   ],
   "source": [
    "# Vector Inner Product\n",
    "vec1 = tensor([1, 2, 3])\n",
    "vec2 = tensor([4, 5, 6])\n",
    "# 벡터의 dot product를 위해서는 torch.dot 메소드 또는 tensor.dot 메소드를 사용합니다.\n",
    "vec3 = vec1.dot(vec2) # 1 * 4 + 2 * 5 + 3 * 6\n",
    "print(vec3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3,  6, -3])\n"
     ]
    }
   ],
   "source": [
    "# Vector Cross Product\n",
    "crosse_vec = torch.cross(vec1, vec2) # 벡터의 길이가 3차원이어야 합니다.\n",
    "print(crosse_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  5,  6],\n",
      "        [ 8, 10, 12],\n",
      "        [12, 15, 18]])\n"
     ]
    }
   ],
   "source": [
    "# Vector Outer Product\n",
    "outer_vec = torch.outer(vec1, vec2) # 벡터의 외적을 계산합니다. 출력은 행렬입니다. \n",
    "print(outer_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4, 10, 18])\n"
     ]
    }
   ],
   "source": [
    "# * : 에스터리스크(asterisk) 이 기호는 element wise product를 수행합니다.\n",
    "vec4 = vec1 * vec2\n",
    "print(vec4) # [4, 10, 18]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5, 12],\n",
      "        [21, 32]])\n"
     ]
    }
   ],
   "source": [
    "# 모든 텐서는 에스터리스크 기호를 통해 원소곱을 수행할 수 있습니다.  \n",
    "mat1 = tensor([[1, 2], \n",
    "              [3, 4]])\n",
    "mat2 = tensor([[5, 6], \n",
    "              [7, 8]])\n",
    "print(mat1 * mat2) # 5, 12, 21, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "tensor([[[  7,  10],\n",
      "         [ 15,  22]],\n",
      "\n",
      "        [[ 67,  78],\n",
      "         [ 91, 106]]])\n",
      "tensor([[[  7,  10],\n",
      "         [ 15,  22]],\n",
      "\n",
      "        [[ 67,  78],\n",
      "         [ 91, 106]]])\n"
     ]
    }
   ],
   "source": [
    "# at sign @ : 골뱅이 기호는 행렬곱을 수행하게 됩니다. \n",
    "# matmul 함수를 통해서도 행렬곱을 계산할 수 있다. \n",
    "\n",
    "mat4 = mat1 @ mat2\n",
    "mat5 = mat1.matmul(mat2)\n",
    "print(mat4)\n",
    "print(mat5)\n",
    "\n",
    "ten1 = tensor([[[1, 2], \n",
    "              [3, 4]],\n",
    "                [[5, 6], \n",
    "              [7, 8]]])\n",
    "ten2 = tensor([[[1, 2], \n",
    "              [3, 4]],\n",
    "                [[5, 6], \n",
    "              [7, 8]]])\n",
    "\n",
    "a = ten1.matmul(ten2)\n",
    "b = ten1 @ ten2\n",
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[-0.7201, -2.3542, -2.1832, -1.1991],\n",
      "        [-0.4095, -1.5249,  1.0711,  0.4676],\n",
      "        [-0.4792, -0.2741, -0.2697,  0.9947],\n",
      "        [ 2.8023,  0.1025,  0.7995, -0.7051]])\n",
      "tensor([-0.7201, -1.5249, -0.2697, -0.7051])\n"
     ]
    }
   ],
   "source": [
    "# 항등행렬 .eye(shape)를 통해 만들 수 있습니다.\n",
    "eye_tensor = torch.eye(2)\n",
    "print(eye_tensor.shape)\n",
    "\n",
    "# 행렬의 대각성분 torch.diag(matrix)\n",
    "v = torch.randn([4,4])\n",
    "print(v)\n",
    "diag_vector = torch.diag(v)\n",
    "print(diag_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50,  60],\n",
      "        [114, 140]])\n"
     ]
    }
   ],
   "source": [
    "# tensor dot도 존재하지만 잘 쓰이지는 않습니다. \n",
    "print(torch.tensordot(ten1, ten2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 * **Tensor dimension manipulation** * (Very important!)  \n",
    "딥러닝 모델에 데이터를 입력하기 위해 텐서의 차원을 정말 잘 다루어야 합니다.   \n",
    "이때 차원을 조작하는 메소드로는 다음과 같습니다.\n",
    "1) 차원 축소, 확장 :  `squeeze(), unsqueeze()` 차원의 크기가 1인 차원을 제거하거나 추가합니다. \n",
    "2) 차원 교환 : `transpose(), permute()` : 차원의 순서를 바꿉니다. 보통 3차원 이상의 텐서를 다룰 때 사용합니다.      \n",
    "3) 텐서 모양 변경 : `flatten(), view(), reshape() ` \n",
    "4) 텐서 합성 및 적재 : `cat(), stack()`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Dimension reduction, extension\n",
    "`tensor.squeeze(axis)`또는 `torch.squeeze(axis)` 함수는 axis 값에 해당하는 특정 축을 제거합니다.\n",
    "`tensor.unsqueeze(axis)` 또는 `torch.unsqueeze(axis)`함수는  axis 값에 해당하는 축에 차원을 추가합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([3, 1, 1])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "vec1 = tensor([1, 2, 3])\n",
    "print(vec1.shape)\n",
    "unsq_vec1 = vec1.unsqueeze(1)\n",
    "print(unsq_vec1.shape)\n",
    "unsq_vec1 = unsq_vec1.unsqueeze(1)\n",
    "print(unsq_vec1.shape)\n",
    "sq_vec1 = unsq_vec1.squeeze(2)\n",
    "print(sq_vec1.shape)\n",
    "sq_vec1 = sq_vec1.squeeze(1)\n",
    "print(sq_vec1.shape)\n",
    "sq_vec1 = sq_vec1.squeeze()\n",
    "print(sq_vec1.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 1, 1, 64, 64, 3])\n",
      "torch.Size([10, 32, 1, 1, 64, 64, 3])\n",
      "torch.Size([10, 32, 1, 64, 64, 3])\n",
      "torch.Size([10, 32, 64, 64, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch.ones(shape)를 통해 모든 값이 1인 텐서를 쉽게 만들 수 있습니다.\n",
    "tensor_7d = torch.ones([10, 32, 1, 1, 64, 64, 3])\n",
    "tensor_7d = torch.zeros([10, 32, 1, 1, 64, 64, 3])\n",
    "\n",
    "\n",
    "# 차원의 모양을 일일히 입력하기 귀찮을 때 like가 붙은 메소드를 사용합니다.\n",
    "tensor_7d2 = torch.ones_like(tensor_7d)\n",
    "tensor_7d3 = torch.zeros_like(tensor_7d)\n",
    "print(tensor_7d.shape)\n",
    "print(tensor_7d2.shape)\n",
    "\n",
    "tensor_6d = tensor_7d.squeeze(axis=2) # shape에서 2번째 인덱스에 해당하는 축을 제거합니다. \n",
    "print(tensor_6d.shape) # [10, 32, 1, 64, 64, 3]\n",
    "tensor_5d = tensor_7d.squeeze()\n",
    "print(tensor_5d.shape) # [10, 32, 64, 64, 3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 64, 64])\n",
      "torch.Size([1, 32, 3, 64, 64])\n",
      "torch.Size([32, 3, 64, 64, 1])\n"
     ]
    }
   ],
   "source": [
    "tensor_4d = torch.ones([32, 3, 64, 64])\n",
    "print(tensor_4d.shape)\n",
    "\n",
    "tensor_5d = tensor_4d.unsqueeze(0)\n",
    "tensor_5d2 = tensor_4d.unsqueeze(4) # = -1 \n",
    "print(tensor_5d.shape)\n",
    "print(tensor_5d2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Dimension exchange: ` transpose(), permute()  `\n",
    "위 메소드들은 차원을 변경할 때 사용합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.ones([2, 3])\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([6, 2, 4, 11])\n"
     ]
    }
   ],
   "source": [
    "# torch.transpose(input, dim0, dim1) 는 인풋 텐서의 두 차원을 교환합니다.\n",
    "# tensor.transpose()를 호출하거나 torch.transpose() 를 통해 사용가능합니다. \n",
    "transposed = torch.transpose(matrix, 0, 1)\n",
    "print(transposed.shape)\n",
    "transposed2 = matrix.transpose(0, 1)\n",
    "print(transposed2.shape)\n",
    "\n",
    "tensor_4d = torch.ones([4, 2, 6, 11])\n",
    "# 퀴즈 : 0번째, 2번째 차원을 교환하세요\n",
    "transposed_4d = tensor_4d.transpose(0, 2)\n",
    "print(transposed_4d.shape) # 6, 2, 4, 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 6, 4, 2])\n",
      "torch.Size([2, 4, 6, 11])\n"
     ]
    }
   ],
   "source": [
    "# torch.permute() : transpose의 상위버전 메소드. 차원 축 순서를 원하는 대로 나열합니다. \n",
    "\n",
    "# 텐서 순서의 조정은 직접 해야 한다.\n",
    "\n",
    "permuted_4d = torch.permute(tensor_4d, [3, 2, 0, 1])\n",
    "print(permuted_4d.shape)\n",
    "permuted_4d2 = torch.permute(tensor_4d, (1, 0, 2, 3))\n",
    "print(permuted_4d2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pop Quiz 2 (Easy)   \n",
    "tensor_4d의 0번째 축에 잉여차원을 추가하고 `permute()`를 사용해 0번째, 2번쨰 차원을 교환하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code implementation\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Changing the shape of tensor  : `flatten(),  reshape(), view()`  \n",
    "위 세 가지 메소드는 주로 CNN 레이어와 FC 레이어 사이에서 데이터를 주고 받을 때 자주 사용합니다.   \n",
    "또는 강화학습에서 입력 텐서의 모양을 변경할 때 자주 사용합니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.flatten()` 메소드는 텐서전체를 1차원 벡터로 변환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "t = tensor([[[1, 2],\n",
    "            [3, 4]],\n",
    "           [[5, 6],\n",
    "           [7, 8]]])\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "flattened_t = torch.flatten(t)\n",
    "print(flattened_t)\n",
    "print(flattened_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 만약에 start_dim, end_dim이 주어지면 해당하는 차원부터 flatten합니다.\n",
    "flattened_t2 = torch.flatten(t, start_dim=2)\n",
    "print(flattened_t2)\n",
    "print(flattened_t2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reshape()` 메소드는 텐서를 원하는 모양을 가지도록 차원을 조정합니다.  \n",
    "단 이때, 각 차원의 총 형태가 원래의 차원과 일치해야 합니다. ex 8 -> 2, 2, 2 or 4, 2  \n",
    "-1을 입력하면 해당차원을 자동으로 계산합니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.arange(36)\n",
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "reshaped_t = t2.reshape(3, 3, 4)\n",
    "print(reshaped_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 2])\n"
     ]
    }
   ],
   "source": [
    "reshaped_t = t2.reshape(-1, 2) \n",
    "print(reshaped_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.view(m, ...,)`: reshape와 동일한 기능을 하지만 메모리 관점에서 다르게 동작합니다. 자세한 내용은 documentation을 참고하세요. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 1247])\n",
      "torch.Size([10, 2494, 1])\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.arange(24940)\n",
    "viewed_t1 = t3.view(5, 4, -1)\n",
    "viewed_t2 = t3.reshape(10, -1, 1)\n",
    "print(viewed_t1.shape)\n",
    "print(viewed_t2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Stacking and Concatenation : `cat() stack()` \n",
    "마지막으로 텐서를 합성하기 위해 cat()과 stack()을 활용합니다.  \n",
    "`cat()` 두 텐서를 동일한 축을 기준으로 합칩니다. 모델 내부에서 다음 은닉층에 들어가는 텐서들을 조합하기 위해 주로 사용합니다.   \n",
    "`stack()` 함수는 데이터를 쌓아서 미니배치를 구성할 때 주로 사용합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "# torch.cat() 함수는 두 개 이상의 텐서 시퀀스들을 (특정 축을 기준으로) 합치게 됩니다. \n",
    "# 해당 텐서들의 차원은 반드시 같아야 합니다. \n",
    "vec1 = tensor([1, 2, 3])\n",
    "vec2 = tensor([4, 5, 6])\n",
    "vec3 = tensor([7, 8, 9])\n",
    "vec4 = torch.cat([vec1, vec2, vec3])\n",
    "print(vec4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "torch.Size([3, 3])\n",
      "tensor([[1, 4, 7],\n",
      "        [2, 5, 8],\n",
      "        [3, 6, 9]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# stack() 메소드는 두 개 이상의 텐서들을 특정 축을 기준으로 쌓게 됩니다.\n",
    "batch_vec = torch.stack([vec1, vec2, vec3], dim=0)\n",
    "print(batch_vec)\n",
    "print(batch_vec.shape)\n",
    "\n",
    "batch_vec = torch.stack([vec1, vec2, vec3], dim=1)\n",
    "print(batch_vec)\n",
    "print(batch_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pop Quiz 3\n",
    "1) 길이가 4096인 랜덤벡터를 하나 생성하세요.  `arange()`\n",
    "2) 벡터를 위에서 배운 메소드들 중 하나를 이용해 (64x64) 정사각행렬로 차원을 조정하세요. `reshape() or view()` \n",
    "3) 1, 2의 과정을 반복해서 정사각행렬을 세 개 만든다음, 각각 텐서의 0번째 축에 크기가 1인 잉여차원을 추가하세요.  `unsqueeze()`\n",
    "4) 0번째 축과 마지막 축을 교환하세요.   `transpose()`\n",
    "5) 리스트에 세 개의 정사각행렬을 담아서 마지막 축을 기준으로 텐서를 합성하세요 -> 64x64x3 텐서가 나와야함  `cat()`\n",
    "6) 위의 과정을 4번 반복해서 64x64x3 를 쌓아서 4 x 64 x 64 x 3 텐서를 만드세요  `stack()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_70292\\1925701815.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1030\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# code :\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1030)\n",
    "# code : \n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pop Quiz 4\n",
    "1. Class.ipynb 에서 제시된 Pop Quiz에서 구현한 Policy Class의 torch의 구성요소로 재구현해보세요. \n",
    "\n",
    "#### Policy class requirements\n",
    "- `__init__(self, state_dim, act_dim)`: \n",
    "  - 기능: state_dimension과 action_dimension을 입력으로 받아서 멤버변수로 저장합니다. \n",
    "  - [aciton_dim, state_dim]의 모양을 가지는 random matrix를 self.weight에 저장합니다. (Hint. `torch.randn` 메소드를 활용하세요)  \n",
    "- `__call__(self, state)`:  \n",
    "  - 기능: get_dist()를 내부적으로 호출하고 그 결과를 리턴합니다.  \n",
    "\n",
    "- `get_dist(self, state)`:\n",
    "  - 이 함수는 state를 입력받아서 distribution을 리턴합니다.   \n",
    "  - self.weight와 state를 행렬곱을 통해 변환된 벡터를 얻습니다. 행렬곱은 `mat3 = mat1 @ mat2` 와 같은 방식으로 계산됩니다. \n",
    "  - 선형변환된 벡터를 nonlinear_vector = `relu(vector)`와 같은 방식으로 비선형함수를 통과시키세요. \n",
    "  - 계산된 벡터를 `softmax` 함수를 적용해서 확률분포 `probs`를 얻습니다.\n",
    "  - `dist=Categorical(probs)` 를 통해 카테고리 분포 `dist`을 얻고 이를 리턴합니다.   \n",
    "  \n",
    "- `get_action(self, dist)`: \n",
    "  - 기능: distribution을 입력받아서 action_index를 샘플링하고 리턴합니다. \n",
    "  - `dist.sample()`를 통해 action_index를 리턴합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax, relu\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        pass\n",
    "    \n",
    "    def get_dist(self, state):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        pass\n",
    "    \n",
    "    def get_action(self, dist):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "obs_dim = 8\n",
    "act_dim = 4\n",
    "policy = Policy(obs_dim, act_dim)\n",
    "\n",
    "state = torch.randn(8)\n",
    "dist = policy(state)\n",
    "action = policy.get_action(dist)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "266dbc1b53b3a75132c42213de358c469c42e7e8486df6091cb08cba7b9be0d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
