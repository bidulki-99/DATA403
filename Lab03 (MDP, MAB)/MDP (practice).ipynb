{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents  \n",
    "- How to check the MDP information\n",
    "- Why do we need Reinforcement Learning?\n",
    "  - Random policy\n",
    "  - Hand-crafted policy\n",
    "- RL policy, Rollout function\n",
    "\n",
    "Writer: KukJin Kim.  \n",
    "Email: kukjinkim@korea.ac.kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\anaconda3\\envs\\rllab\\lib\\site-packages (1.24.2)\n",
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     - -------------------------------------- 30.7/721.7 kB ? eta -:--:--\n",
      "     ------ ------------------------------- 122.9/721.7 kB 2.4 MB/s eta 0:00:01\n",
      "     ---------- --------------------------- 204.8/721.7 kB 1.8 MB/s eta 0:00:01\n",
      "     ------------- ------------------------ 256.0/721.7 kB 2.2 MB/s eta 0:00:01\n",
      "     ------------------- ------------------ 368.6/721.7 kB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------ ------- 583.7/721.7 kB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------ ------- 583.7/721.7 kB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 721.7/721.7 kB 2.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\anaconda3\\envs\\rllab\\lib\\site-packages (from gym) (6.0.0)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Collecting cloudpickle>=1.2.0\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\anaconda3\\envs\\rllab\\lib\\site-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827647 sha256=1f81a2705eda2174311b6604d688c5825a0c0a83b95ba9ea82b0f29c59d1637d\n",
      "  Stored in directory: c:\\users\\bdk\\appdata\\local\\pip\\cache\\wheels\\17\\79\\65\\7afedc162d858b02708a3b8f7a6dd5b1000dcd5b0f894f7cc1\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-2.2.1 gym-0.26.2 gym-notices-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import gym\n",
    "from gym.spaces import Discrete\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. First step for RL: Check the MDP information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-03-27-18-51-41.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, we learned that reinforcement learning is a kind of black box that takes MDP as input and outputs optimal policies.  \n",
    "The first step for reinforcement learning is to define an MDP or to check the information in the defined MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP is defined as follows. $<\\cal {S, A, P, R}, \\gamma>$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-03-27-20-33-27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know the reward and the transition probability here, we say we know the information about the environment.  \n",
    "Then we can do planning. We can obtain the best policy without having to interact with an environment.  \n",
    "For most problems in reality, the transition probability is unknown. If we don't know the environment, we can try a model-free approach.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The core information we use is the dimension of the state space (observation space) and the action space.  \n",
    "We can know the state and action infomation of the environment by env.observation_space and env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FrozenLake environment\n",
    "![](2023-03-27-20-51-22.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# state (observation) and action space information\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's guess the information of other environments\n",
    "### CartPole\n",
    "State: angle of a pole, x location, velocity, raw image, or a combination of above factors  \n",
    "Action: move left, move right\n",
    "\n",
    "![](2023-03-27-20-52-19.png)  \n",
    "\n",
    "### Pendulum\n",
    "State: angle of a pendulum, angular velocity, raw image, or a combination of above factors \n",
    "Action: rotate left, rotate right, amount of force\n",
    "\n",
    "![](2023-03-27-20-52-42.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the state and action dimension by using env.observation_space and env.action_space ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Discrete(2)\n",
      "\n",
      "Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "Box(-2.0, 2.0, (1,), float32)\n"
     ]
    }
   ],
   "source": [
    "env2 = gym.make('CartPole-v1')\n",
    "### print observation_space and action_space\n",
    "print(env2.observation_space)\n",
    "print(env2.action_space)\n",
    "\n",
    "print()\n",
    "\n",
    "env3 = gym.make('Pendulum-v1')\n",
    "### print observation_space and action_space\n",
    "print(env3.observation_space)\n",
    "print(env3.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the output? If you look at the information in CartPole and Pendulum, you can see that observation space and action space are configured as objects called Box.  \n",
    "Observation and action can be discrete or continuous. In continuous cases, it is represented through the Box object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Why do we need Reinforcement Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll see why reinforcement learning is useful in sequential decision making.  \n",
    "Let's assume that we want to solve the simple gridworld environment that we learned in class  \n",
    "The implementations of the environment are below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self, env_config=None, *args, **kwargs) -> None:\n",
    "        self.grid_size = env_config['grid_size']\n",
    "        self.out_reward = env_config['out_reward']\n",
    "        self.step_reward = env_config['step_reward']\n",
    "        self.goal_reward = env_config['goal_reward']\n",
    "        self.max_step = env_config['max_step']\n",
    "        self.goal = env_config['goal']\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(self.grid_size*self.grid_size)\n",
    "        self.seeker = (0, 0)\n",
    "        self.info = {'seeker': self.seeker, 'goal': self.goal}\n",
    "        self.timestep = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.seeker = (0, 0) # row, col\n",
    "        self.timestep = 0\n",
    "        return self.get_observation()\n",
    "    \n",
    "    def get_observation(self):\n",
    "        return self.grid_size * self.seeker[0] + self.seeker[1]\n",
    "    \n",
    "    def get_reward(self):\n",
    "        return self.goal_reward if self.seeker == self.goal else 0\n",
    "    \n",
    "    def is_done(self):\n",
    "        if self.timestep == self.max_step:\n",
    "            return True\n",
    "        return self.seeker == self.goal\n",
    "\n",
    "    def check_pos(self, seeker):\n",
    "        is_out = False\n",
    "        if seeker[0] < 0 or seeker[0] > self.grid_size - 1 or \\\n",
    "            seeker[1] < 0 or seeker[1] > self.grid_size - 1: \n",
    "            is_out = True\n",
    "        return is_out\n",
    "\n",
    "    def step(self, action):\n",
    "        self.timestep += 1\n",
    "        reward = 0\n",
    "        is_out = False\n",
    "\n",
    "        if action == 0: # move left\n",
    "            self.seeker = (self.seeker[0], self.seeker[1] - 1)\n",
    "            is_out =  self.check_pos(self.seeker)\n",
    "            if is_out:\n",
    "                self.seeker = (self.seeker[0], self.seeker[1] + 1)\n",
    "\n",
    "        elif action == 1: # move right\n",
    "            self.seeker = (self.seeker[0], self.seeker[1] + 1)\n",
    "            is_out =  self.check_pos(self.seeker)\n",
    "            if is_out:\n",
    "                self.seeker = (self.seeker[0], self.seeker[1] - 1)\n",
    "\n",
    "        elif action == 2: # move up\n",
    "            self.seeker = (self.seeker[0] - 1, self.seeker[1])\n",
    "            is_out =  self.check_pos(self.seeker)\n",
    "            if is_out:\n",
    "                self.seeker = (self.seeker[0] + 1, self.seeker[1])\n",
    "                \n",
    "        elif action == 3: # move down\n",
    "            self.seeker = (self.seeker[0] + 1, self.seeker[1])\n",
    "            is_out =  self.check_pos(self.seeker)\n",
    "            if is_out:\n",
    "                self.seeker = (self.seeker[0] - 1, self.seeker[1])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        if is_out:\n",
    "            reward = self.out_reward\n",
    "        else:\n",
    "            reward = self.get_reward() + self.step_reward\n",
    "\n",
    "        return self.get_observation(), reward, self.is_done(), self.info\n",
    "\n",
    "    def render(self, *args, **kwaargs):\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')\n",
    "        clear_output(wait=True)\n",
    "        grid_row = ['| ' for _ in range(self.grid_size)] \n",
    "        grid = [grid_row + [\"|\\n\"] for _ in range(self.grid_size)]\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.seeker[0]][self.seeker[1]] = '|A'\n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))\n",
    "\n",
    "\n",
    "class Gridworld(Environment, gym.Env):\n",
    "    def __init__(self, env_config, *args, **kwargs) -> None:\n",
    "        super().__init__(env_config=env_config, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid size, goal reward, and max_step are defined in dictionaries. We can change the environment by modifying these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action:  \n",
    "0: LEFT  \n",
    "1: RIGHT  \n",
    "2: UP   \n",
    "3: DOWN  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "class Action(IntEnum):\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "    UP = 2\n",
    "    DOWN = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|A| | | |\n",
      "| | | | |\n",
      "| | | | |\n",
      "| | | |G|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_config = {\"grid_size\": 4,\n",
    "              \"goal\": (3, 3),\n",
    "             \"goal_reward\": 1,\n",
    "             \"step_reward\": -1,\n",
    "             \"out_reward\": -5,\n",
    "             \"max_step\": 20 }\n",
    "env = Gridworld(env_config)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| |A| | |\n",
      "| | | | |\n",
      "| | | | |\n",
      "| | | |G|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.step(Action.RIGHT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | | |\n",
      "| |A| | |\n",
      "| | | | |\n",
      "| | | |G|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.step(Action.DOWN)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Random policy  \n",
    "We learned that we need a policy to solve decision problems.  \n",
    "First, you need to implement the policy that behave randomly in your environment.  \n",
    "Random policy is considered as baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop Quiz 1. Implement the random_policy function\n",
    "- random_policy function returns random action (0~3)\n",
    "- To return a random action, utilize the `random` built-in library` and action information\n",
    "- You can get the size of action dimension by using `env.action_space.n`\n",
    "- Hint: use `random.randint(min, max)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_policy(env):\n",
    "    ### Implement here ###\n",
    "    action = random.randint(0, env.action_space.n - 1)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below block to see whether your random_policy works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | | |\n",
      "| | | | |\n",
      "| | | | |\n",
      "| | | |A|\n",
      "\n",
      "Action.RIGHT\n",
      "current episode: 1\n",
      "steps: 8\n",
      "agent pos. x:  3, y:  3\n",
      "goal pos.  x:  3, y:  3\n",
      "total reward: -7\n"
     ]
    }
   ],
   "source": [
    "render_time = 0.5\n",
    "render = True\n",
    "for episode in range(0, 2):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    current_step = 0\n",
    "    while not done:\n",
    "        action = random_policy(env)\n",
    "        # ! You can also use the env.action_space.sample() function\n",
    "        # action = env.action_space.sample()\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        current_step += 1\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if render:\n",
    "            time.sleep(render_time)\n",
    "            env.render()\n",
    "            print(Action(action))\n",
    "            print(f\"current episode: {episode}\")\n",
    "            print(f\"steps: {current_step}\")\n",
    "            print(f\"agent pos. x: {env.seeker[0]:>2d}, y: {env.seeker[1]:>2d}\")\n",
    "            print(f\"goal pos.  x: {env.goal[0]:>2d}, y: {env.goal[1]:>2d}\")\n",
    "            print(f\"total reward: {total_reward:>2d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Hand-crafted policy   \n",
    "In the above code block, the random policy mostly does not reach the destination well.  \n",
    "We know that the optimal policy should be made as shown in the figure below. Let's implement hand-crafted policy!  \n",
    "\n",
    "![](2023-03-27-22-16-45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop Quiz 2. Implement the hand-crafted policy\n",
    "- Hand-crafted policy takes state as input (0~14) and returns a optimal action (0~3)\n",
    "- To return the optimal action, utilize the `if, elif, else` conditional statement and enum class `Action`\n",
    "- Use `random.choice(list)` built-in function for the cases (0, 1, 2, 4, 5, 6, 8, 9, 10) to return random action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "class Action(IntEnum):\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "    UP = 2\n",
    "    DOWN = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action.RIGHT\n"
     ]
    }
   ],
   "source": [
    "action_list = [Action.RIGHT, Action.DOWN]\n",
    "action = random.choice(action_list)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_crafted_policy(state):\n",
    "    if state in [0, 1, 2, 4, 5, 6, 8, 9, 10]:\n",
    "        action = random.choice(action_list)\n",
    "    elif state in [3, 7, 11]:\n",
    "        action = Action.DOWN\n",
    "    else:\n",
    "        action = Action.RIGHT\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below block to see whether your hand_crafted_policy works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | | |\n",
      "| | | | |\n",
      "| | | | |\n",
      "| | | |A|\n",
      "\n",
      "Action.RIGHT\n",
      "current episode: 9\n",
      "steps: 6\n",
      "agent pos. x:  3, y:  3\n",
      "goal pos.  x:  3, y:  3\n",
      "total reward: -5\n"
     ]
    }
   ],
   "source": [
    "render_time = 0.1\n",
    "render = True\n",
    "for episode in range(0, 10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    current_step = 0\n",
    "    while not done:\n",
    "        action = hand_crafted_policy(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        current_step += 1\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if render:\n",
    "            time.sleep(render_time)\n",
    "            env.render()\n",
    "            print(Action(action))\n",
    "            print(f\"current episode: {episode}\")\n",
    "            print(f\"steps: {current_step}\")\n",
    "            print(f\"agent pos. x: {env.seeker[0]:>2d}, y: {env.seeker[1]:>2d}\")\n",
    "            print(f\"goal pos.  x: {env.goal[0]:>2d}, y: {env.goal[1]:>2d}\")\n",
    "            print(f\"total reward: {total_reward:>2d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented it well, your policy will be performed successfully in a gridworld environment.  \n",
    "But let's think about the following situation. Grid size is increased doubly and destination location is moved to left side.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|A| | | | | | | |\n",
      "| | | | | | | | |\n",
      "| | | | | | | | |\n",
      "| | | | | | | | |\n",
      "| | | | | | | | |\n",
      "| | | | | | | | |\n",
      "| | | | | | | | |\n",
      "|G| | | | | | | |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_config = {\"grid_size\": 8,\n",
    "              \"goal\": (7, 0),\n",
    "             \"goal_reward\": 1,\n",
    "             \"step_reward\": -1,\n",
    "             \"out_reward\": -5,\n",
    "             \"max_step\": 50 }\n",
    "env = Gridworld(env_config)\n",
    "env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|A| | | | | | | | | | | | | | | |\n",
      "| | | | | | | | | | | | | | | | |\n",
      "| | | | | | | | | | | | | | | | |\n",
      "| | | | | | | | | | | | | | | | |\n",
      "| | | | | | | | | | | | | | | | |\n",
      "| | | | | | | | | | | | | | | | |\n",
      "| | | | | | | | | | | | | | | | |\n",
      "| | | | | | | | | | | | | | | | |\n",
      "| | | | | | | | | | | | | | | | |\n",
      "| | | | | | | | | | | | | | | | |\n",
      "| | | | | | | | | | | | | | | | |\n",
      "| | | | | | | | | | | | | | | | |\n",
      "| | | | | | | | | | | | | | | | |\n",
      "| | | | | | | | | | | |G| | | | |\n",
      "| | | | | | | | | | | | | | | | |\n",
      "| | | | | | | | | | | | | | | | |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_config = {\"grid_size\": 16,\n",
    "              \"goal\": (13, 11),\n",
    "             \"goal_reward\": 1,\n",
    "             \"step_reward\": -1,\n",
    "             \"out_reward\": -5,\n",
    "             \"max_step\": 250 }\n",
    "env = Gridworld(env_config)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Of course, we can create manual policies for changing environments, but we can't solve every problem in reality in this way because of limited time and cost. If we require to solve high-dimensional or continuous domain problem, we need an automated methodology.   \n",
    "### That's reinforcement learning!! ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. RL policy, Rollout function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will practice Q-Learning of reinforcement learning. You don't have to understand everything because you'll soon learn the detailed principles in future lecture.  \n",
    "So let's implement RL Policy. In the above code blocks, the policy is implemented as a function such like `random_policy` and `hand_crafted_policy`  \n",
    "But policy is typically implemented through `stateful class`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, env): # Q-table\n",
    "        self.state_action_table = [\n",
    "            [0.0 for _ in range(env.action_space.n)] for _ in range(env.observation_space.n)\n",
    "            ]\n",
    "        self.action_space = env.action_space\n",
    "    \n",
    "    def get_action(self, state, explore=True, epsilon=0.1): # epsilon-greedy\n",
    "        if explore and random.uniform(0,1) < epsilon:\n",
    "            return self.action_space.sample()\n",
    "        return np.argmax(self.state_action_table[state])\n",
    "\n",
    "    def save(self, num): \n",
    "        name = f\"policy{num}.npy\"\n",
    "        np.save(name, self.state_action_table)\n",
    "    \n",
    "    def load(self, npy_path):\n",
    "         self.state_action_table = np.load(npy_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce code repetition, let's make the abstract function of the the agent interactions.  \n",
    "We will make the `rollout` function that is the process of getting samples while the agent interacts with the environment.  \n",
    "The rollout function takes input such as policy, environment, and maximum length of episode and then returns the agent's trajectory.  \n",
    "\n",
    "![](2023-03-27-23-21-30.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, policy, T, *args, **kwargs):\n",
    "    render = kwargs[\"render\"]\n",
    "    epsilon = kwargs[\"epsilon\"]\n",
    "    render_time = kwargs[\"render_time\"]\n",
    "    explore = kwargs[\"explore\"]\n",
    "    episode = kwargs[\"episode\"]\n",
    "    \n",
    "    experiences = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    current_step = 0\n",
    "    while not done:\n",
    "        action = policy.get_action(state, explore, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        experiences.append([state, action, reward, next_state, done])\n",
    "        current_step += 1\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if render:\n",
    "            time.sleep(render_time)\n",
    "            env.render()\n",
    "            print(Action(action))            \n",
    "            print(f\"current episode: {episode}\")\n",
    "            print(f\"current step: {current_step}\")\n",
    "            print(f\"agent pos. x: {env.seeker[0]:>2d}, y: {env.seeker[1]:>2d}\")\n",
    "            print(f\"goal pos.  x: {env.goal[0]:>2d}, y: {env.goal[1]:>2d}\")\n",
    "            print(f\"total reward: {total_reward:>2d}\")\n",
    "            \n",
    "        if current_step == T:\n",
    "            break\n",
    "    return experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update_policy: Update the policy by using Q-learning equation\n",
    "#### train_policy: Iterate update_policy over the episodes.\n",
    "#### evaluate_policy: Evaluate and visualize to see if the learning went well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(policy, trajectory, weight=0.1, discount_factor=0.9):\n",
    "    random.shuffle(trajectory)\n",
    "    for state, action, reward, next_state, done in trajectory:\n",
    "        next_max = np.max(policy.state_action_table[next_state])\n",
    "        value = policy.state_action_table[state][action]\n",
    "        new_value = (1 - weight) * value + weight * (reward + discount_factor * next_max)\n",
    "        policy.state_action_table[state][action] = new_value\n",
    "\n",
    "def train_policy(env, policy, T=20, num_episodes=10000, weight=0.1, discount_factor=0.9, epsilon=0.5):\n",
    "    for e in range(1, num_episodes+1):\n",
    "        trajectory = rollout(env, policy, T, render=False, render_time=0.0, explore=True, epsilon=epsilon, episode=e)\n",
    "        update_policy(policy, trajectory, weight, discount_factor)\n",
    "        \n",
    "    policy.save(e)\n",
    "    return policy\n",
    "\n",
    "def evaluate_policy(env, policy, T, npy_path, num_episodes=5):\n",
    "    policy.load(npy_path)\n",
    "    steps = 0\n",
    "    total_reward_lst = []\n",
    "    avg_score = 0\n",
    "    for e in range(num_episodes):\n",
    "        experiences = rollout(env, policy, T, render=True, render_time=0.1, explore=False, epsilon=0, episode=e)\n",
    "        total_reward = 0\n",
    "        for transition in experiences:\n",
    "            total_reward += transition[2]\n",
    "        total_reward_lst.append(total_reward)\n",
    "        steps += len(experiences)\n",
    "    \n",
    "    avg_score = sum(total_reward_lst) / len(total_reward_lst)\n",
    "    return steps / num_episodes, avg_score, total_reward_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|A| | | | | | | |\n",
      "| | | | | | | | |\n",
      "| | | | | | | | |\n",
      "| | | | | | | | |\n",
      "| | | | | | | | |\n",
      "| | | | | | | | |\n",
      "| | | | | | |G| |\n",
      "| | | | | | | | |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_config = {\"grid_size\": 8,\n",
    "              \"goal\": (6, 6),\n",
    "             \"goal_reward\": 1,\n",
    "             \"step_reward\": -1,\n",
    "             \"out_reward\": -5,\n",
    "             \"max_step\": 40 }\n",
    "env = Gridworld(env_config)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-11.18  -6.86 -11.18  -6.86]\n",
      " [ -7.18  -6.51 -10.86  -6.51]\n",
      " [ -6.86  -6.13 -10.51  -6.13]\n",
      " [ -6.51  -5.7  -10.13  -5.7 ]\n",
      " [ -6.13  -5.22  -9.7   -5.22]\n",
      " [ -5.7   -4.69  -9.22  -4.69]\n",
      " [ -5.22  -5.22  -8.69  -4.1 ]\n",
      " [ -4.69  -9.22  -9.22  -4.69]\n",
      " [-10.86  -6.51  -7.18  -6.51]\n",
      " [ -6.86  -6.13  -6.86  -6.13]\n",
      " [ -6.51  -5.7   -6.51  -5.7 ]\n",
      " [ -6.13  -5.22  -6.13  -5.22]\n",
      " [ -5.7   -4.69  -5.7   -4.69]\n",
      " [ -5.22  -4.1   -5.22  -4.1 ]\n",
      " [ -4.69  -4.69  -4.69  -3.44]\n",
      " [ -4.1   -8.69  -5.22  -4.1 ]\n",
      " [-10.51  -6.13  -6.86  -6.13]\n",
      " [ -6.51  -5.7   -6.51  -5.7 ]\n",
      " [ -6.13  -5.22  -6.13  -5.22]\n",
      " [ -5.7   -4.69  -5.7   -4.69]\n",
      " [ -5.22  -4.1   -5.22  -4.1 ]\n",
      " [ -4.69  -3.44  -4.69  -3.44]\n",
      " [ -4.1   -4.1   -4.1   -2.71]\n",
      " [ -3.44  -8.1   -4.69  -3.44]\n",
      " [-10.13  -5.7   -6.51  -5.7 ]\n",
      " [ -6.13  -5.22  -6.13  -5.22]\n",
      " [ -5.7   -4.69  -5.7   -4.69]\n",
      " [ -5.22  -4.1   -5.22  -4.1 ]\n",
      " [ -4.69  -3.44  -4.69  -3.44]\n",
      " [ -4.1   -2.71  -4.1   -2.71]\n",
      " [ -3.44  -3.44  -3.44  -1.9 ]\n",
      " [ -2.71  -7.44  -4.1   -2.71]\n",
      " [ -9.7   -5.22  -6.13  -5.22]\n",
      " [ -5.7   -4.69  -5.7   -4.69]\n",
      " [ -5.22  -4.1   -5.22  -4.1 ]\n",
      " [ -4.69  -3.44  -4.69  -3.44]\n",
      " [ -4.1   -2.71  -4.1   -2.71]\n",
      " [ -3.44  -1.9   -3.44  -1.9 ]\n",
      " [ -2.71  -2.71  -2.71  -1.  ]\n",
      " [ -1.9   -6.71  -3.44  -1.9 ]\n",
      " [ -9.22  -4.69  -5.7   -4.69]\n",
      " [ -5.22  -4.1   -5.22  -4.1 ]\n",
      " [ -4.69  -3.44  -4.69  -3.44]\n",
      " [ -4.1   -2.71  -4.1   -2.71]\n",
      " [ -3.44  -1.9   -3.44  -1.9 ]\n",
      " [ -2.71  -1.    -2.71  -1.  ]\n",
      " [ -1.9   -1.9   -1.9    0.  ]\n",
      " [ -1.    -5.9   -2.71  -1.  ]\n",
      " [ -8.68  -4.1   -5.22  -5.2 ]\n",
      " [ -4.69  -3.44  -4.69  -4.69]\n",
      " [ -4.1   -2.71  -4.1   -4.1 ]\n",
      " [ -3.44  -1.9   -3.44  -3.44]\n",
      " [ -2.71  -1.    -2.71  -2.71]\n",
      " [ -1.9    0.    -1.9   -1.9 ]\n",
      " [  0.     0.     0.     0.  ]\n",
      " [  0.    -5.    -1.9   -1.9 ]\n",
      " [ -8.25  -4.68  -4.68  -8.69]\n",
      " [ -4.97  -4.1   -4.1   -8.48]\n",
      " [ -4.66  -3.44  -3.44  -8.09]\n",
      " [ -4.09  -2.71  -2.71  -7.43]\n",
      " [ -3.44  -1.9   -1.9   -6.71]\n",
      " [ -2.71  -1.    -1.    -5.9 ]\n",
      " [ -1.89  -1.82   0.    -4.97]\n",
      " [ -1.    -5.62  -1.    -4.95]]\n"
     ]
    }
   ],
   "source": [
    "untrained_policy = Policy(env)\n",
    "trained_policy = train_policy(env, untrained_policy, env_config[\"max_step\"], num_episodes=10000)\n",
    "print(np.round(trained_policy.state_action_table, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| | | | | | | | |\n",
      "| | | | | | | | |\n",
      "| | | | | | | | |\n",
      "| | | | | | | | |\n",
      "| | | | | | | | |\n",
      "| | | | | | | | |\n",
      "| | | | | | |A| |\n",
      "| | | | | | | | |\n",
      "\n",
      "Action.DOWN\n",
      "current episode: 4\n",
      "current step: 12\n",
      "agent pos. x:  6, y:  6\n",
      "goal pos.  x:  6, y:  6\n",
      "total reward: -11\n"
     ]
    }
   ],
   "source": [
    "avg_steps, avg_score, total_reward_lst = evaluate_policy(env, trained_policy, env_config[\"max_step\"], \"policy10000.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_steps: 12.0, avg_score: -11.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"avg_steps: {avg_steps}, avg_score: {avg_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c6276770e9c5a685155d297a75ee9fe2216c271da1ff18b89edbaa80fe1643b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
